{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddb93f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import spatialdata as sd\n",
    "from skimage.measure import regionprops\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from geomloss import SamplesLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from config import DATA_PATH\n",
    "from data import patch_john, patch_harry\n",
    "from models import utils as model_utils\n",
    "from eval import utils as eval_utils\n",
    "\n",
    "# hugginface\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "load_dotenv(dotenv_path=os.path.expanduser('~/hl/.gutinstinct.env'))\n",
    "api_token = os.getenv(\"API_TOKEN\")\n",
    "login(token=api_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8c58edb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'eval.utils' has no attribute 'compute_r2_spearman'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# eval_utils.l1_normalize(1)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43meval_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_r2_spearman\u001b[49m(y_true_all, y_pred_all)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'eval.utils' has no attribute 'compute_r2_spearman'"
     ]
    }
   ],
   "source": [
    "# eval_utils.l1_normalize(1)\n",
    "eval_utils.compute_r2_spearman(y_true_all, y_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e76d450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b-evelyntong/miniconda3/envs/gutinstinct/lib/python3.11/site-packages/zarr/creation.py:610: UserWarning: ignoring keyword argument 'read_only'\n",
      "  compressor, fill_value = _kwargs_compat(compressor, fill_value, kwargs)\n"
     ]
    }
   ],
   "source": [
    "zarr_path = osp.join(os.path.expanduser(DATA_PATH), \"UC6_I.zarr/UC6_I.zarr\")\n",
    "sdata = sd.read_zarr(zarr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f756982",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_patch_train, dataset_patch_val, dataset_patch_test = patch_harry.get_patches(sdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36cc8966",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_expression_train = model_utils.get_expression(sdata, dataset_patch_train)\n",
    "dataset_expression_val = model_utils.get_expression(sdata, dataset_patch_val)\n",
    "dataset_expression_test = model_utils.get_expression(sdata, dataset_patch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a2aab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset_patch_train, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(dataset_patch_val, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset_patch_test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e16770c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 1536, kernel_size=(32, 32), stride=(32, 32))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (12): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (13): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (14): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (15): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (16): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (17): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (18): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (19): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (20): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (21): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (22): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (23): Block(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): GluMlp(\n",
       "        (fc1): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (act): SiLU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## instantiate a uni model\n",
    "timm_kwargs = {\n",
    "   'img_size': 224,\n",
    "   'patch_size': 32,\n",
    "   'depth': 24,\n",
    "   'num_heads': 24,\n",
    "   'init_values': 1e-5,\n",
    "   'embed_dim': 1536,\n",
    "   'mlp_ratio': 2.66667*2,\n",
    "   'num_classes': 0,\n",
    "   'no_embed_class': True,\n",
    "   'mlp_layer': timm.layers.SwiGLUPacked,\n",
    "   'act_layer': torch.nn.SiLU,\n",
    "   'reg_tokens': 8,\n",
    "   'dynamic_img_size': True\n",
    "  }\n",
    "model_uni = timm.create_model(\"hf-hub:MahmoodLab/UNI2-h\", pretrained=True, **timm_kwargs)\n",
    "model_uni = model_uni.to('cuda')\n",
    "transform = create_transform(**resolve_data_config(model_uni.pretrained_cfg, model=model_uni))\n",
    "model_uni.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65253897",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings, train_cell_ids = model_utils.extract_features(train_loader, model_uni)\n",
    "val_embeddings, val_cell_ids = model_utils.extract_features(val_loader, model_uni)\n",
    "test_embeddings, test_cell_ids = model_utils.extract_features(test_loader, model_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c6350fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(train_embeddings, dtype=torch.float32),\n",
    "    torch.tensor(dataset_expression_train, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "val_dataset = TensorDataset(\n",
    "    torch.tensor(val_embeddings, dtype=torch.float32),\n",
    "    torch.tensor(dataset_expression_val, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "test_dataset = TensorDataset(\n",
    "    torch.tensor(test_embeddings, dtype=torch.float32),\n",
    "    torch.tensor(dataset_expression_test, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "# create dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5796da0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'eval.utils' has no attribute 'l1_normalize'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m optimizer.zero_grad()\n\u001b[32m     32\u001b[39m y_pred = model(x_batch)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m y_pred_dist = \u001b[43meval_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43ml1_normalize\u001b[49m(y_pred)\n\u001b[32m     34\u001b[39m y_true_dist = eval_utils.l1_normalize(y_batch)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# use both point-wise and distributional loss\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: module 'eval.utils' has no attribute 'l1_normalize'"
     ]
    }
   ],
   "source": [
    "# fit fcn model\n",
    "feature_dim = train_embeddings.shape[1]\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(feature_dim, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 460)\n",
    ")\n",
    "model.to('cuda')\n",
    "\n",
    "# some setups\n",
    "criterion = SamplesLoss(\"sinkhorn\", p=2, blur=0.05)\n",
    "lmd = 0.7        # controls how much we care about the point-wise loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# es logic\n",
    "es = None\n",
    "best_r2 = float('inf')\n",
    "patience = 5\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to('cuda'), y_batch.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_batch)\n",
    "        y_pred_dist = eval_utils.l1_normalize(y_pred)\n",
    "        y_true_dist = eval_utils.l1_normalize(y_batch)\n",
    "\n",
    "        # use both point-wise and distributional loss\n",
    "        mse_loss = F.mse_loss(y_pred, y_batch)\n",
    "        dist_loss = criterion(l1_normalize(y_pred), l1_normalize(y_batch))\n",
    "        loss = lmd * mse_loss + (1 - lmd) * dist_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "    train_losses.append(running_train_loss / len(train_loader))\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    y_true_all, y_pred_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val, y_val = x_val.to('cuda'), y_val.to('cuda')\n",
    "            y_pred_val = model(x_val)\n",
    "            y_pred_dist_val = eval_utils.l1_normalize(y_pred_val, dim=1)\n",
    "            y_true_dist_val = eval_utils.l1_normalize(y_val, dim=1)\n",
    "\n",
    "            # use both point-wise and distributional loss\n",
    "            mse_loss = F.mse_loss(y_pred_val, y_val)\n",
    "            dist_loss = criterion(y_pred_dist_val, y_true_dist_val)\n",
    "            val_loss = lmd * mse_loss + (1 - lmd) * dist_loss\n",
    "\n",
    "            running_val_loss += val_loss.item()\n",
    "\n",
    "            y_true_all.append(y_val.detach().cpu().numpy())\n",
    "            y_pred_all.append(y_pred_val.detach().cpu().numpy())\n",
    "\n",
    "    val_losses.append(running_val_loss / len(val_loader))\n",
    "    val_r2, val_spearman = eval_utils.compute_r2_spearman(y_true_all, y_pred_all)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Val R2: {val_r2:.4f}, Val Spearman: {val_spearman:.4f}\")\n",
    "\n",
    "    if es is not None:\n",
    "        if val_r2 < best_r2:\n",
    "            best_r2 = val_r2\n",
    "            counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gutinstinct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
